{"total":16,"pageSize":10,"pageCount":2,"current":2,"data":[{"title":"论文速读《Learning to compose words into sentences with reinforcement learning》","date":"2016-11-13T16:00:00.000Z","excerpt":"<h2 id=\"链接\"><a href=\"#链接\" class=\"headerlink\" title=\"链接\"></a><a href=\"https://openreview.net/forum?id=Skvgqgqxe\" target=\"_blank\" rel=\"noopener\">链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>google</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Tree-LSTM, Reinforcement Learning</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>ICLR 2017</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>使用强化学习来构建树结构的神经网络Tree-LSTM，学习自然语言的句子表示</p>","slug":"Learning to compose words into sentences with reinforcement learning_Tonya","tags":["RL"],"categories":["论文"]},{"title":"论文速读《Two are Better than One An Ensemble of Retrieval and Generation-Based Dialog》","date":"2016-11-09T16:00:00.000Z","excerpt":"","slug":"Two are  Better than One An Ensemble of Retrieval and Generation-Based Dialog","tags":["RL","NLG"]},{"title":"论文速读《Deep Reinforcement Learning for Dialogue Generation》","date":"2016-11-09T16:00:00.000Z","excerpt":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1606.01541\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Jiwei Li</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>斯坦福</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Reinforcement Learning, seq2seq, text generation</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>arXiv.org(2016.06.25) &amp; EMNLP2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>本文提出利用强化学习进行开放领域的文本生成任务，并对比了有监督的seq2seq加attention模型和基于最大互信息的模型</p>","slug":"Deep Reinforcement Learning for Dialogue Generation","tags":["NLP","RL","chatbot"],"categories":["论文"]},{"title":"论文速读《On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems》","date":"2016-11-01T16:00:00.000Z","excerpt":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1605.07669\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>剑桥 </p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>对话系统、强化学习、在线主动奖励学习(On-line Active Reward Learning)</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>ACL 2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>文章提出一种在线学习框架，通过高斯过程分类模型进行主动学习，训练对话策略和奖励模型，减少数据标注的花费和用户反馈中的噪声。<br>","slug":"On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems","tags":["RL","chatbot"],"categories":["论文"]},{"title":"机器学习","date":"2015-12-20T12:10:16.000Z","excerpt":"","slug":"机器学习","tags":["机器学习"],"categories":["求知"]},{"title":"reading","date":"2015-12-20T09:08:47.000Z","excerpt":"","slug":"reading"}]}