{"total":14,"pageSize":10,"pageCount":2,"current":2,"data":[{"title":"powerline配置","date":"2016-11-09T16:00:00.000Z","excerpt":"","slug":"powerline配置","tags":["配置"],"categories":["其他"]},{"title":"论文速读《Deep Reinforcement Learning for Dialogue Generation》","date":"2016-11-09T16:00:00.000Z","excerpt":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1606.01541\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Jiwei Li</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>斯坦福</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Reinforcement Learning, seq2seq, text generation</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>arXiv.org(2016.06.25) &amp; EMNLP2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>本文提出利用强化学习进行开放领域的文本生成任务，并对比了有监督的seq2seq加attention模型和基于最大互信息的模型</p>","slug":"Deep Reinforcement Learning for Dialogue Generation","tags":["NLP","RL","chatbot"],"categories":["论文"]},{"title":"论文速读《Two are Better than One An Ensemble of Retrieval and Generation-Based Dialog》","date":"2016-11-09T16:00:00.000Z","excerpt":"","slug":"Two are  Better than One An Ensemble of Retrieval and Generation-Based Dialog","tags":["RL","NLG"]},{"title":"论文速读《On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems》","date":"2016-11-01T16:00:00.000Z","excerpt":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1605.07669\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>剑桥 </p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>对话系统、强化学习、在线主动奖励学习(On-line Active Reward Learning)</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>ACL 2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>文章提出一种在线学习框架，通过高斯过程分类模型进行主动学习，训练对话策略和奖励模型，减少数据标注的花费和用户反馈中的噪声。<br>","slug":"On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems","tags":["RL","chatbot"],"categories":["论文"]}]}