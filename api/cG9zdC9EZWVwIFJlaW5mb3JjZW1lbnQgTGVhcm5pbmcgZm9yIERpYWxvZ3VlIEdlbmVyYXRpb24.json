{"title":"论文速读《Deep Reinforcement Learning for Dialogue Generation》","date":"2016-11-09T16:00:00.000Z","excerpt":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1606.01541\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Jiwei Li</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>斯坦福</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Reinforcement Learning, seq2seq, text generation</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>arXiv.org(2016.06.25) &amp; EMNLP2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>本文提出利用强化学习进行开放领域的文本生成任务，并对比了有监督的seq2seq加attention模型和基于最大互信息的模型</p>","slug":"Deep Reinforcement Learning for Dialogue Generation","comments":true,"tags":["NLP","RL","chatbot"],"categories":["论文"],"updated":"2016-11-26T15:43:07.000Z","content":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1606.01541\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Jiwei Li</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>斯坦福</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Reinforcement Learning, seq2seq, text generation</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>arXiv.org(2016.06.25) &amp; EMNLP2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>本文提出利用强化学习进行开放领域的文本生成任务，并对比了有监督的seq2seq加attention模型和基于最大互信息的模型</p>\n<a id=\"more\"></a>\n<h2 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h2><p>强化学习中的reward</p>\n<p><img src=\"http://7xpodp.com1.z0.glb.clouddn.com/blog/20161123/231524231.png\" alt=\"\"></p>\n<p>易被响应（Ease of answering），不容易出现对话僵局，其中 S 是无意义回答合集，s是某一时刻的响应</p>\n<p><img src=\"http://7xpodp.com1.z0.glb.clouddn.com/blog/20161123/231624954.png\" alt=\"\"></p>\n<p>信息流，若开辟新的话题，有利于对话的继续发展，隐层表示 hpi 和 hpi+1 的夹角余弦</p>\n<p><img src=\"http://7xpodp.com1.z0.glb.clouddn.com/blog/20161123/231655514.png\" alt=\"\"></p>\n<p>语义连贯性，减少与对话无关问题的影响，其中，pseq2seq(a|pi,qi) 是由上一轮状态得到响应的概率，后一项是由当前产生响应通过网络生成之前的 qi 的概率。</p>\n<p>最终的reward是对三者加权求和，系数分别为：0.25、0.25、0.5<br><img src=\"http://7xpodp.com1.z0.glb.clouddn.com/blog/20161123/231723685.png\" alt=\"\"><br>对比试验：</p>\n<ol>\n<li>对话初始状态为一个SEQ2SEQ加attention的模型作为强化学习的初始状态</li>\n<li>在1的基础上将最大互信息加入其中作为reward，对于一个给定的输入[pi,qi]，可以根据模型生成一个候选回答集合A。对于A中的每一个回答a,从预训练模型中得到的概率分布上可以计算出互信息的值 m(a,[pi,qi])。</li>\n<li>将互信息训练过的模型作为初始模型，用策略梯度更新参数并加入课程学习策略，最终最多限定五轮对话</li>\n</ol>\n<h2 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h2><p><img src=\"http://7xpodp.com1.z0.glb.clouddn.com/DQN_related.png\" alt=\"DQN相关\"></p>\n<h2 id=\"简评\"><a href=\"#简评\" class=\"headerlink\" title=\"简评\"></a>简评</h2><p>本文的思想其实非常符合写作的一种情况，就像贾岛推敲的故事，回想小时候刚学习写句子时，也不能一次写好，总会不断对一些词语进行修改。Google DeepMind的文章《DRAW：A Recurrent Neural Network For Image》也和本文异曲同工：画画也不是一次画好，也要不断的完善。不同之处在于本文率先引入DQN做文本生成。在机器学习各个分支下，强化学习和人类与环境的交互方式非常相似，在许多领域开始初露头角，期待看到更多将强化学习结合语言模型的应用。</p>\n","prev":{"title":"论文速读《Learning to compose words into sentences with reinforcement learning》","slug":"Learning to compose words into sentences with reinforcement learning_Tonya"},"next":{"title":"powerline配置","slug":"powerline配置"},"link":"http://yoursite.com/post/Deep Reinforcement Learning for Dialogue Generation","toc":[{"title":"作者","id":"作者","index":"1"},{"title":"单位","id":"单位","index":"2"},{"title":"关键词","id":"关键词","index":"3"},{"title":"文章来源","id":"文章来源","index":"4"},{"title":"问题","id":"问题","index":"5"},{"title":"模型","id":"模型","index":"6"},{"title":"相关工作","id":"相关工作","index":"7"},{"title":"简评","id":"简评","index":"8"}]}