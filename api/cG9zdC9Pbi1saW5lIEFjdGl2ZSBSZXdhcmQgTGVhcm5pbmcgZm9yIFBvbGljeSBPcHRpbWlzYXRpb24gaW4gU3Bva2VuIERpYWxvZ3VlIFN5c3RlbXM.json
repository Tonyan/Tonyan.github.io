{"title":"论文速读《On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems》","date":"2016-11-01T16:00:00.000Z","excerpt":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1605.07669\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>剑桥 </p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>对话系统、强化学习、在线主动奖励学习(On-line Active Reward Learning)</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>ACL 2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>文章提出一种在线学习框架，通过高斯过程分类模型进行主动学习，训练对话策略和奖励模型，减少数据标注的花费和用户反馈中的噪声。<br>","slug":"On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems","comments":true,"tags":["RL","chatbot"],"categories":["论文"],"updated":"2016-11-26T15:56:24.000Z","content":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1605.07669\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>剑桥 </p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>对话系统、强化学习、在线主动奖励学习(On-line Active Reward Learning)</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>ACL 2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>文章提出一种在线学习框架，通过高斯过程分类模型进行主动学习，训练对话策略和奖励模型，减少数据标注的花费和用户反馈中的噪声。<br><a id=\"more\"></a></p>\n<h2 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h2><p><img src=\"http://7xpodp.com1.z0.glb.clouddn.com/blog/20161123/231954501.png\" alt=\"\"><br>框架分为三部分：对话策略、对话嵌入函数、用户反馈主动奖励模型<br>无监督学习输入为双向LSTM，通过Encoder-Decoder模型表征用户意图，将对话的成功与否看做高斯过程的一个二元分类问题，当模型对当前结果不能评判时，主动学习，通过reward模型决定是否询问用户反馈，当模型不确定时，生成增强信号来训练策略。</p>\n<h2 id=\"资源\"><a href=\"#资源\" class=\"headerlink\" title=\"资源\"></a>资源</h2><p>Dataset是在Amazon Mechanical Turk上收集的语料，具体应用在为剑桥地区的一个餐馆电话服务系统进行验证，模型应用的Theano，未开源代码和数据集。</p>\n<h2 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h2><p>-之前的工作有用任务完成度和对话持续情况做Reward，但任务完成度不好衡量<br>-用协同过滤表征用户偏好<br>-用逆强化学习从行为中推出reward</p>\n<h2 id=\"简评\"><a href=\"#简评\" class=\"headerlink\" title=\"简评\"></a>简评</h2><p>用lSTM Encoder-Decoder表征用户意图，无需大规模标注语料和构建用户模拟器来进行训练，在较小的训练语料中取得了不错的效果，率先实现了在真实场景中的应用。但Reward函数只关心对话任务是否成功，模型过于简单。</p>\n","prev":{"title":"论文速读《Deep Reinforcement Learning for Dialogue Generation》","slug":"Deep Reinforcement Learning for Dialogue Generation"},"link":"http://yoursite.com/post/On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems","toc":[{"title":"作者","id":"作者","index":"1"},{"title":"单位","id":"单位","index":"2"},{"title":"关键词","id":"关键词","index":"3"},{"title":"文章来源","id":"文章来源","index":"4"},{"title":"问题","id":"问题","index":"5"},{"title":"模型","id":"模型","index":"6"},{"title":"资源","id":"资源","index":"7"},{"title":"相关工作","id":"相关工作","index":"8"},{"title":"简评","id":"简评","index":"9"}]}