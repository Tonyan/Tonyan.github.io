{"total":11,"pageSize":10,"pageCount":2,"current":1,"data":[{"title":"numpy 安装学习","date":"2017-06-20T12:10:16.000Z","excerpt":"","slug":"numpy安装学习","tags":["机器学习"],"categories":["求知"]},{"title":"论文速读《Learning to Generate Reviews and Discovering Sentiment》","date":"2017-04-23T16:00:00.000Z","excerpt":"<h2 id=\"链接\"><a href=\"#链接\" class=\"headerlink\" title=\"链接\"></a><a href=\"https://arxiv.org/abs/1704.01444\" target=\"_blank\" rel=\"noopener\">链接</a></h2><p>##摘要<br>我们提出了一种byte-level的RNN language model。当给出足够的空间、训练数据、计算时间，通过该模型的表示包括disentangled features（不依赖特征？）和高水平的概念一致？<br>特别地，我们发现了一个单独的情感分析单元。这些表示学习了一个无监督的方式，并在斯坦福情感分析树图资料库SST的二元分类上取得了最好的结果。而且该模型数据有效？只用一部分的标注数据和在所有数据上取得的baseline相当。我们也证明了该情感单元对于模型的生成结果有直接影响。简单的修改他的值为积极或者消极生成的样例即为积极或者消极的情感。</p>\n<p>##简介和动机<br>表示学习在目前机器学习系统中扮演者关键的角色。表示将原始数据映射成更实用的形式，并且表示学习的选择对于任何应用都是至关重要的成分。更一般的说，有两个领域的研究强调如何学习有用表示的不同细节。<br>有标注的大规模语料高维度的监督学习训练是非常非常重要的</p>","slug":"Learning to Generate Reviews and Discovering Sentiment","tags":["sentiment","genetate"],"categories":["论文"]},{"title":"A Simple, Fast Diverse Decoding Algorithm for Neural Generation","date":"2016-12-20T12:10:16.000Z","excerpt":"","slug":"A Simple,Fast Diverse Decoding Algorithm for Neural Generation_Tonya","tags":["NLP","RL","chatbot"],"categories":["论文"]},{"title":"github page博客不同电脑管理","date":"2016-11-24T16:00:00.000Z","excerpt":"<p>目前本地已有github page所有文件，想直接建立两个分支分别存储源文件和生成好的博客<br>发现直接git新建分支不好用，此时在git中切换至存放博客的主目录执行,目的在于将当前文件初始化为git可管理文件<br><code>git init</code><br>由于刚刚初始化为git目录，现在识别不出master分支，会报错“fatal: Not a valid object name: ‘master’”<br>-解决该问题需要提交一次才可以，即<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add.</span><br><span class=\"line\">git commit -m &quot;commit&quot;</span><br></pre></td></tr></table></figure></p>","slug":"GithubPage不同电脑管理","tags":["技术"],"categories":["配置"]},{"title":"tensorflow-HPC配置方法","date":"2016-11-21T16:00:00.000Z","excerpt":"","slug":"tensorflow-HPC配置方法","tags":["技术"],"categories":["技术"]},{"title":"论文速读《Learning to compose words into sentences with reinforcement learning》","date":"2016-11-13T16:00:00.000Z","excerpt":"<h2 id=\"链接\"><a href=\"#链接\" class=\"headerlink\" title=\"链接\"></a><a href=\"https://openreview.net/forum?id=Skvgqgqxe\" target=\"_blank\" rel=\"noopener\">链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>google</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Tree-LSTM, Reinforcement Learning</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>ICLR 2017</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>使用强化学习来构建树结构的神经网络Tree-LSTM，学习自然语言的句子表示</p>","slug":"Learning to compose words into sentences with reinforcement learning_Tonya","tags":["RL"],"categories":["论文"]},{"title":"论文速读《Deep Reinforcement Learning for Dialogue Generation》","date":"2016-11-09T16:00:00.000Z","excerpt":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1606.01541\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Jiwei Li</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>斯坦福</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Reinforcement Learning, seq2seq, text generation</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>arXiv.org(2016.06.25) &amp; EMNLP2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>本文提出利用强化学习进行开放领域的文本生成任务，并对比了有监督的seq2seq加attention模型和基于最大互信息的模型</p>","slug":"Deep Reinforcement Learning for Dialogue Generation","tags":["NLP","RL","chatbot"],"categories":["论文"]},{"title":"论文速读《Two are Better than One An Ensemble of Retrieval and Generation-Based Dialog》","date":"2016-11-09T16:00:00.000Z","excerpt":"","slug":"Two are  Better than One An Ensemble of Retrieval and Generation-Based Dialog","tags":["RL","NLG"]},{"title":"powerline配置","date":"2016-11-09T16:00:00.000Z","excerpt":"","slug":"powerline配置","tags":["配置"],"categories":["其他"]},{"title":"论文速读《On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems》","date":"2016-11-01T16:00:00.000Z","excerpt":"<h2 id=\"原文链接\"><a href=\"#原文链接\" class=\"headerlink\" title=\"原文链接\"></a><a href=\"https://arxiv.org/abs/1605.07669\" target=\"_blank\" rel=\"noopener\">原文链接</a></h2><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>剑桥 </p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>对话系统、强化学习、在线主动奖励学习(On-line Active Reward Learning)</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p>ACL 2016</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>文章提出一种在线学习框架，通过高斯过程分类模型进行主动学习，训练对话策略和奖励模型，减少数据标注的花费和用户反馈中的噪声。<br>","slug":"On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems","tags":["RL","chatbot"],"categories":["论文"]}]}