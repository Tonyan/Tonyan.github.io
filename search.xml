<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>预训练语言模型梳理</title>
    <url>/2020/02/29/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%A2%B3%E7%90%86/</url>
    <content><![CDATA[<h6 id="自然语言处理范式总结"><a href="#自然语言处理范式总结" class="headerlink" title="自然语言处理范式总结"></a>自然语言处理范式总结</h6><p>范式就是规范的意思，在自然语言处理领域目前总共可分为四个里程碑及四个范式，分别为：</p>
<p><img src="https://www.msra.cn/wp-content/uploads/2020/01/2019-review-nlp-speech-1.png" alt=""></p>
<ol>
<li>第一个范式为1990年以前的处理方法，基本为字典／词典辅以规则</li>
<li>第二个范式主要是2000-2012年的统计机器学习模型</li>
<li>第三个范式则是以深度学习为代表的模型，端到端的神经网络模型</li>
<li>第四个范式是以预训练语言模型加微调为主，再进行模型压缩</li>
</ol>
<a id="more"></a>
<h4 id="详尽的预处理模型关系图"><a href="#详尽的预处理模型关系图" class="headerlink" title="详尽的预处理模型关系图"></a>详尽的预处理模型关系图</h4><p><img src="https://pic1.zhimg.com/v2-447ae7707604e7ac520555249332c42c_1200x500.jpg" alt=""></p>
<ol>
<li>预训练语言模型</li>
</ol>
<p>模型可以通过监督学习方法获得，这是最常规的统计学习思路</p>
<p>今年来涌现的预训练语言模型则可利用三种不同类型的数据学习得到模型：</p>
<ul>
<li>生文本：通过自监督学习（self-supervised learning）得到预训练模型</li>
<li>辅助任务标注数据：通过预训练（pre-training）得到预训练模型</li>
<li>标注数据：通过精调（fine-tuning）得到预训练模型</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Tonyan/BlogPicBed/master//img/预训练模型.png" alt=""></p>
<h4 id="词向量时代"><a href="#词向量时代" class="headerlink" title="词向量时代"></a>词向量时代</h4><h4 id="预训练语言模型时代"><a href="#预训练语言模型时代" class="headerlink" title="预训练语言模型时代"></a>预训练语言模型时代</h4><h5 id="预处理范式时间轴"><a href="#预处理范式时间轴" class="headerlink" title="预处理范式时间轴"></a>预处理范式时间轴</h5><p><img src="https://www.msra.cn/wp-content/uploads/2020/01/2019-review-nlp-speech-2.png" alt=""></p>
<h5 id="各种预处理语言模型发展关系图"><a href="#各种预处理语言模型发展关系图" class="headerlink" title="各种预处理语言模型发展关系图"></a>各种预处理语言模型发展关系图</h5><p><img src="https://static.leiphone.com/uploads/new/images/20191031/5dba851bec6e1.png?imageView2/2/w/740" alt=""></p>
<h4 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h4><ol>
<li><a href="http://ir.hit.edu.cn/~car/talks/pre-training4nlp.pdf" target="_blank" rel="noopener">预训练模型-自然语言处理的新范式 </a></li>
<li><a href="https://www.msra.cn/zh-cn/news/features/2019-review-nlp-speech" target="_blank" rel="noopener">NLP新范式凸显跨任务、跨语言能力，语音处理落地开花</a></li>
<li><a href="https://www.leiphone.com/news/201910/VIRhfQaQERfRedvB.html" target="_blank" rel="noopener">NLP领域预训练模型的现状及分析</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/93781241" target="_blank" rel="noopener">预训练语言模型(PLMs)走的飞快</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/85221503" target="_blank" rel="noopener">NLP预训练模型：从transformer到albert</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/76912493" target="_blank" rel="noopener">nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)</a></li>
<li><a href="http://www.xuwei.io/2018/11/20/nlp的巨人肩膀/" target="_blank" rel="noopener">NLP的巨人肩膀</a></li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>深度学习系列</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
